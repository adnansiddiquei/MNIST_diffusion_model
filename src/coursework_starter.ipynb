{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOqtixLa1y-b"
   },
   "source": [
    "The following example notebook implements standard diffusion\n",
    "with a simple CNN model to generate realistic MNIST digits.\n",
    "\n",
    "This is a modified implementation of `minDiffusion`\n",
    "which implements [DDPM](https://arxiv.org/abs/2006.11239)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this example notebook,\n",
    "install requirements as in `requirements.txt` (for example, `pip install -r requirements.txt`).\n",
    "You may also wish to follow system-dependent PyTorch instructions\n",
    "[here](https://pytorch.org/) to install accelerated\n",
    "versions of PyTorch, but note they are not needed\n",
    "(I am testing this on my laptop).\n",
    "\n",
    "If you do use accelerated hardware, make sure that your code\n",
    "is still compatible with CPU-only installs.\n",
    "\n",
    "First, let's create a folder to store example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaJ7P2ft2G6j",
    "outputId": "7ce57688-755a-431b-c73d-2e32301824ea",
    "ExecuteTime": {
     "end_time": "2024-03-19T00:22:30.674660Z",
     "start_time": "2024-03-19T00:22:30.540248Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50FGtZsk1y-b",
    "ExecuteTime": {
     "end_time": "2024-03-21T16:22:55.192099Z",
     "start_time": "2024-03-21T16:22:53.575736Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from utils import ddpm_schedules, CNNBlock, CNN, DDPM, save_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this on MNIST. We perform some basic preprocessing, and set up the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a6jMrCRa1y-d",
    "ExecuteTime": {
     "end_time": "2024-03-21T16:22:55.210978Z",
     "start_time": "2024-03-21T16:22:55.192997Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the train function."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_ddpm(\n",
    "        ddpm: nn.Module, \n",
    "        optim: torch.optim.Optimizer, \n",
    "        dataloader: DataLoader, \n",
    "        accelerator: Accelerator, \n",
    "        save_folder: str,\n",
    "        n_epoch: int = 100,\n",
    "        start_epoch: int = 0\n",
    "    ):\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_epoch):\n",
    "        ddpm.train()\n",
    "    \n",
    "        pbar = tqdm(dataloader)  # Wrap our loop with a visual progress bar\n",
    "        for x, _ in pbar:\n",
    "            optim.zero_grad()\n",
    "    \n",
    "            loss = ddpm(x)\n",
    "    \n",
    "            loss.backward()\n",
    "            # ^Technically should be `accelerator.backward(loss)` but not necessary for local training\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "            avg_loss = np.average(losses[min(len(losses)-100, 0):])\n",
    "            pbar.set_description(f\"loss: {avg_loss:.3g}\")  # Show running average of loss in progress bar\n",
    "    \n",
    "            optim.step()\n",
    "    \n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            xh = ddpm.sample(16, (1, 28, 28), accelerator.device)  # Can get device explicitly with `accelerator.device`\n",
    "            grid = make_grid(xh, nrow=4)\n",
    "    \n",
    "            # Save samples to `./contents` directory\n",
    "            save_image(grid, f\"./{save_folder}/ddpm_sample_{start_epoch + i:04d}.png\")\n",
    "    \n",
    "            # save model\n",
    "            torch.save(ddpm.state_dict(), f\"./{save_folder}/ddpm_mnist_{start_epoch + i}.pth\")\n",
    "            \n",
    "    save_pickle(losses, f\"./{save_folder}/ddpm_mnist_losses_{start_epoch}.pkl\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T17:28:58.772070Z",
     "start_time": "2024-03-21T17:28:58.767280Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we define each of the models we will be testing.\n",
    "\n",
    "We essentially just vary the noise schedule but keep it linear in all of them. We keep the initial noise the same but change the final noise so that we add noise at a quicker or slower rate.\n",
    "\n",
    "Here, we use HuggingFace's `accelerate` library, which abstracts away all the `.to(device)` calls for us.\n",
    "This lets us focus on the model itself rather than data movement.\n",
    "It also does a few other tricks to speed up calculations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# this is the default model\n",
    "accelerator_1 = Accelerator()\n",
    "gt_1 = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "ddpm_1 = DDPM(gt=gt_1, betas=(1e-4, 0.02), n_T=1000)\n",
    "optim_1 = torch.optim.Adam(ddpm_1.parameters(), lr=2e-4)\n",
    "ddpm_1, optim_1, dataloader_1 = accelerator_1.prepare(ddpm_1, optim_1, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T17:39:12.244938Z",
     "start_time": "2024-03-21T17:39:12.226982Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accelerator_2 = Accelerator()\n",
    "gt_2 = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "ddpm_2 = DDPM(gt=gt_2, betas=(1e-4, 0.1), n_T=200)\n",
    "optim_2 = torch.optim.Adam(ddpm_2.parameters(), lr=2e-4)\n",
    "ddpm_2, optim_2, dataloader_2 = accelerator_2.prepare(ddpm_2, optim_2, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T17:39:12.525475Z",
     "start_time": "2024-03-21T17:39:12.507482Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "accelerator_3 = Accelerator()\n",
    "gt_3 = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "ddpm_3 = DDPM(gt=gt_3, betas=(1e-4, 0.004), n_T=5000)\n",
    "optim_3 = torch.optim.Adam(ddpm_3.parameters(), lr=2e-4)\n",
    "ddpm_3, optim_3, dataloader_3 = accelerator_3.prepare(ddpm_3, optim_3, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T17:39:12.937712Z",
     "start_time": "2024-03-21T17:39:12.922644Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train the models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.114: 100%|██████████| 468/468 [00:29<00:00, 15.67it/s]\n",
      "loss: 0.0804: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]\n",
      "loss: 0.0666: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0586: 100%|██████████| 468/468 [00:28<00:00, 16.29it/s]\n",
      "loss: 0.0532: 100%|██████████| 468/468 [00:28<00:00, 16.35it/s]\n",
      "loss: 0.0492: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0461: 100%|██████████| 468/468 [00:28<00:00, 16.44it/s]\n",
      "loss: 0.0437: 100%|██████████| 468/468 [00:28<00:00, 16.29it/s]\n",
      "loss: 0.0417: 100%|██████████| 468/468 [00:28<00:00, 16.53it/s]\n",
      "loss: 0.04: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]  \n",
      "loss: 0.0385: 100%|██████████| 468/468 [00:28<00:00, 16.38it/s]\n",
      "loss: 0.0373: 100%|██████████| 468/468 [00:28<00:00, 16.40it/s]\n",
      "loss: 0.0362: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0352: 100%|██████████| 468/468 [00:28<00:00, 16.57it/s]\n",
      "loss: 0.0343: 100%|██████████| 468/468 [00:28<00:00, 16.54it/s]\n",
      "loss: 0.0335: 100%|██████████| 468/468 [00:28<00:00, 16.47it/s]\n",
      "loss: 0.0328: 100%|██████████| 468/468 [00:28<00:00, 16.22it/s]\n",
      "loss: 0.0322: 100%|██████████| 468/468 [00:28<00:00, 16.54it/s]\n",
      "loss: 0.0316: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]\n",
      "loss: 0.0311: 100%|██████████| 468/468 [00:28<00:00, 16.55it/s]\n",
      "loss: 0.0306: 100%|██████████| 468/468 [00:28<00:00, 16.56it/s]\n",
      "loss: 0.0301: 100%|██████████| 468/468 [00:28<00:00, 16.46it/s]\n",
      "loss: 0.0297: 100%|██████████| 468/468 [00:28<00:00, 16.30it/s]\n",
      "loss: 0.0292: 100%|██████████| 468/468 [00:28<00:00, 16.50it/s]\n",
      "loss: 0.0289: 100%|██████████| 468/468 [00:28<00:00, 16.50it/s]\n",
      "loss: 0.0285: 100%|██████████| 468/468 [00:27<00:00, 17.00it/s]\n",
      "loss: 0.0282: 100%|██████████| 468/468 [00:27<00:00, 16.90it/s]\n",
      "loss: 0.0279: 100%|██████████| 468/468 [00:27<00:00, 16.77it/s]\n",
      "loss: 0.0276: 100%|██████████| 468/468 [00:28<00:00, 16.70it/s]\n",
      "loss: 0.0273: 100%|██████████| 468/468 [00:29<00:00, 16.12it/s]\n",
      "loss: 0.0271: 100%|██████████| 468/468 [00:28<00:00, 16.17it/s]\n",
      "loss: 0.0268: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0266: 100%|██████████| 468/468 [00:28<00:00, 16.44it/s]\n",
      "loss: 0.0264: 100%|██████████| 468/468 [00:28<00:00, 16.45it/s]\n",
      "loss: 0.0261: 100%|██████████| 468/468 [00:28<00:00, 16.23it/s]\n",
      "loss: 0.0259: 100%|██████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "loss: 0.0258: 100%|██████████| 468/468 [00:29<00:00, 16.00it/s]\n",
      "loss: 0.0256: 100%|██████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "loss: 0.0254: 100%|██████████| 468/468 [00:29<00:00, 15.99it/s]\n",
      "loss: 0.0252: 100%|██████████| 468/468 [00:29<00:00, 16.10it/s]\n",
      "loss: 0.0251: 100%|██████████| 468/468 [00:29<00:00, 16.13it/s]\n",
      "loss: 0.0249: 100%|██████████| 468/468 [00:29<00:00, 16.12it/s]\n",
      "loss: 0.0248: 100%|██████████| 468/468 [00:29<00:00, 15.96it/s]\n",
      "loss: 0.0246: 100%|██████████| 468/468 [00:29<00:00, 15.90it/s]\n",
      "loss: 0.0245: 100%|██████████| 468/468 [00:29<00:00, 15.87it/s]\n",
      "loss: 0.0243: 100%|██████████| 468/468 [00:29<00:00, 15.83it/s]\n",
      "loss: 0.0242: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0241: 100%|██████████| 468/468 [00:29<00:00, 15.87it/s]\n",
      "loss: 0.024: 100%|██████████| 468/468 [00:29<00:00, 15.82it/s] \n",
      "loss: 0.0238: 100%|██████████| 468/468 [00:29<00:00, 15.79it/s]\n",
      "loss: 0.0237: 100%|██████████| 468/468 [00:29<00:00, 15.85it/s]\n",
      "loss: 0.0236: 100%|██████████| 468/468 [00:29<00:00, 15.78it/s]\n",
      "loss: 0.0235: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s]\n",
      "loss: 0.0234: 100%|██████████| 468/468 [00:29<00:00, 15.81it/s]\n",
      "loss: 0.0233: 100%|██████████| 468/468 [00:29<00:00, 15.78it/s]\n",
      "loss: 0.0232: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0231: 100%|██████████| 468/468 [00:29<00:00, 15.88it/s]\n",
      "loss: 0.023: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s] \n",
      "loss: 0.0229: 100%|██████████| 468/468 [00:29<00:00, 15.75it/s]\n",
      "loss: 0.0229: 100%|██████████| 468/468 [00:29<00:00, 15.85it/s]\n",
      "loss: 0.0228: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s]\n",
      "loss: 0.0227: 100%|██████████| 468/468 [00:29<00:00, 15.85it/s]\n",
      "loss: 0.0226: 100%|██████████| 468/468 [00:29<00:00, 15.85it/s]\n",
      "loss: 0.0225: 100%|██████████| 468/468 [00:29<00:00, 15.82it/s]\n",
      "loss: 0.0224: 100%|██████████| 468/468 [00:29<00:00, 15.88it/s]\n",
      "loss: 0.0224: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s]\n",
      "loss: 0.0223: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0222: 100%|██████████| 468/468 [00:29<00:00, 15.83it/s]\n",
      "loss: 0.0222: 100%|██████████| 468/468 [00:29<00:00, 15.85it/s]\n",
      "loss: 0.0221: 100%|██████████| 468/468 [00:29<00:00, 15.81it/s]\n",
      "loss: 0.022: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s] \n",
      "loss: 0.022: 100%|██████████| 468/468 [00:29<00:00, 15.81it/s]\n",
      "loss: 0.0219: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0218: 100%|██████████| 468/468 [00:29<00:00, 15.78it/s]\n",
      "loss: 0.0218: 100%|██████████| 468/468 [00:29<00:00, 15.81it/s]\n",
      "loss: 0.0217: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0217: 100%|██████████| 468/468 [00:29<00:00, 15.83it/s]\n",
      "loss: 0.0216: 100%|██████████| 468/468 [00:29<00:00, 15.78it/s]\n",
      "loss: 0.0216: 100%|██████████| 468/468 [00:29<00:00, 15.82it/s]\n",
      "loss: 0.0215: 100%|██████████| 468/468 [00:29<00:00, 15.75it/s]\n",
      "loss: 0.0214: 100%|██████████| 468/468 [00:29<00:00, 15.76it/s]\n",
      "loss: 0.0214: 100%|██████████| 468/468 [00:29<00:00, 15.69it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:28<00:00, 16.20it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0212: 100%|██████████| 468/468 [00:28<00:00, 16.54it/s]\n",
      "loss: 0.0212: 100%|██████████| 468/468 [00:28<00:00, 16.48it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:28<00:00, 16.47it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:28<00:00, 16.57it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:28<00:00, 16.55it/s]\n",
      "loss: 0.021: 100%|██████████| 468/468 [00:28<00:00, 16.58it/s] \n",
      "loss: 0.021: 100%|██████████| 468/468 [00:28<00:00, 16.56it/s]\n",
      "loss: 0.0209: 100%|██████████| 468/468 [00:28<00:00, 16.56it/s]\n",
      "loss: 0.0209: 100%|██████████| 468/468 [00:28<00:00, 16.54it/s]\n",
      "loss: 0.0208: 100%|██████████| 468/468 [00:28<00:00, 16.50it/s]\n",
      "loss: 0.0208: 100%|██████████| 468/468 [00:28<00:00, 16.56it/s]\n",
      "loss: 0.0208: 100%|██████████| 468/468 [00:28<00:00, 16.59it/s]\n",
      "loss: 0.0207: 100%|██████████| 468/468 [00:28<00:00, 16.53it/s]\n",
      "loss: 0.0207: 100%|██████████| 468/468 [00:28<00:00, 16.44it/s]\n",
      "loss: 0.0207: 100%|██████████| 468/468 [00:28<00:00, 16.53it/s]\n",
      "loss: 0.0206: 100%|██████████| 468/468 [00:29<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p contents_2\n",
    "train_ddpm(ddpm_2, optim_2, dataloader_2, accelerator_2, \"contents_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T18:28:30.271765Z",
     "start_time": "2024-03-21T17:39:18.538561Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.124: 100%|██████████| 468/468 [00:28<00:00, 16.39it/s]\n",
      "loss: 0.0841: 100%|██████████| 468/468 [00:27<00:00, 16.83it/s]\n",
      "loss: 0.068: 100%|██████████| 468/468 [00:27<00:00, 16.78it/s] \n",
      "loss: 0.0589: 100%|██████████| 468/468 [00:27<00:00, 16.75it/s]\n",
      "loss: 0.0528: 100%|██████████| 468/468 [00:30<00:00, 15.49it/s]\n",
      "loss: 0.0486: 100%|██████████| 468/468 [00:29<00:00, 15.98it/s]\n",
      "loss: 0.0452: 100%|██████████| 468/468 [00:28<00:00, 16.31it/s]\n",
      "loss: 0.0427: 100%|██████████| 468/468 [00:28<00:00, 16.66it/s]\n",
      "loss: 0.0405: 100%|██████████| 468/468 [00:29<00:00, 15.96it/s]\n",
      "loss: 0.0387: 100%|██████████| 468/468 [00:29<00:00, 15.97it/s]\n",
      "loss: 0.0372: 100%|██████████| 468/468 [00:29<00:00, 15.89it/s]\n",
      "loss: 0.0359: 100%|██████████| 468/468 [00:29<00:00, 15.87it/s]\n",
      "loss: 0.0348: 100%|██████████| 468/468 [00:28<00:00, 16.19it/s]\n",
      "loss: 0.0338: 100%|██████████| 468/468 [00:28<00:00, 16.21it/s]\n",
      "loss: 0.0329: 100%|██████████| 468/468 [00:29<00:00, 16.10it/s]\n",
      "loss: 0.0321: 100%|██████████| 468/468 [00:28<00:00, 16.16it/s]\n",
      "loss: 0.0314: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0308: 100%|██████████| 468/468 [00:29<00:00, 16.06it/s]\n",
      "loss: 0.0302: 100%|██████████| 468/468 [00:29<00:00, 16.11it/s]\n",
      "loss: 0.0296: 100%|██████████| 468/468 [00:28<00:00, 16.40it/s]\n",
      "loss: 0.0291: 100%|██████████| 468/468 [00:28<00:00, 16.33it/s]\n",
      "loss: 0.0286: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]\n",
      "loss: 0.0282: 100%|██████████| 468/468 [00:28<00:00, 16.44it/s]\n",
      "loss: 0.0278: 100%|██████████| 468/468 [00:28<00:00, 16.46it/s]\n",
      "loss: 0.0274: 100%|██████████| 468/468 [00:28<00:00, 16.39it/s]\n",
      "loss: 0.0271: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0267: 100%|██████████| 468/468 [00:28<00:00, 16.31it/s]\n",
      "loss: 0.0264: 100%|██████████| 468/468 [00:28<00:00, 16.46it/s]\n",
      "loss: 0.0261: 100%|██████████| 468/468 [00:27<00:00, 17.10it/s]\n",
      "loss: 0.0258: 100%|██████████| 468/468 [00:27<00:00, 16.78it/s]\n",
      "loss: 0.0256: 100%|██████████| 468/468 [00:28<00:00, 16.40it/s]\n",
      "loss: 0.0253: 100%|██████████| 468/468 [00:28<00:00, 16.33it/s]\n",
      "loss: 0.0251: 100%|██████████| 468/468 [00:28<00:00, 16.64it/s]\n",
      "loss: 0.0248: 100%|██████████| 468/468 [00:28<00:00, 16.66it/s]\n",
      "loss: 0.0246: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0244: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0242: 100%|██████████| 468/468 [00:28<00:00, 16.63it/s]\n",
      "loss: 0.024: 100%|██████████| 468/468 [00:28<00:00, 16.71it/s] \n",
      "loss: 0.0238: 100%|██████████| 468/468 [00:30<00:00, 15.44it/s]\n",
      "loss: 0.0237: 100%|██████████| 468/468 [00:28<00:00, 16.14it/s]\n",
      "loss: 0.0235: 100%|██████████| 468/468 [00:29<00:00, 16.01it/s]\n",
      "loss: 0.0233: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0232: 100%|██████████| 468/468 [00:29<00:00, 16.13it/s]\n",
      "loss: 0.023: 100%|██████████| 468/468 [00:29<00:00, 15.96it/s] \n",
      "loss: 0.0229: 100%|██████████| 468/468 [00:28<00:00, 16.45it/s]\n",
      "loss: 0.0228: 100%|██████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "loss: 0.0226: 100%|██████████| 468/468 [00:28<00:00, 16.31it/s]\n",
      "loss: 0.0225: 100%|██████████| 468/468 [00:28<00:00, 16.48it/s]\n",
      "loss: 0.0224: 100%|██████████| 468/468 [00:28<00:00, 16.26it/s]\n",
      "loss: 0.0223: 100%|██████████| 468/468 [00:28<00:00, 16.60it/s]\n",
      "loss: 0.0222: 100%|██████████| 468/468 [00:27<00:00, 16.87it/s]\n",
      "loss: 0.022: 100%|██████████| 468/468 [00:27<00:00, 16.76it/s] \n",
      "loss: 0.0219: 100%|██████████| 468/468 [00:28<00:00, 16.66it/s]\n",
      "loss: 0.0218: 100%|██████████| 468/468 [00:29<00:00, 16.02it/s]\n",
      "loss: 0.0217: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s]\n",
      "loss: 0.0216: 100%|██████████| 468/468 [00:29<00:00, 15.95it/s]\n",
      "loss: 0.0215: 100%|██████████| 468/468 [00:29<00:00, 15.84it/s]\n",
      "loss: 0.0214: 100%|██████████| 468/468 [00:29<00:00, 15.86it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:30<00:00, 15.55it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:29<00:00, 15.90it/s]\n",
      "loss: 0.0212: 100%|██████████| 468/468 [00:29<00:00, 15.83it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:29<00:00, 15.78it/s]\n",
      "loss: 0.021: 100%|██████████| 468/468 [00:29<00:00, 15.90it/s] \n",
      "loss: 0.0209: 100%|██████████| 468/468 [00:29<00:00, 15.81it/s]\n",
      "loss: 0.0209: 100%|██████████| 468/468 [00:29<00:00, 15.86it/s]\n",
      "loss: 0.0208: 100%|██████████| 468/468 [00:35<00:00, 13.27it/s]\n",
      "loss: 0.0207: 100%|██████████| 468/468 [00:30<00:00, 15.57it/s]\n",
      "loss: 0.0206: 100%|██████████| 468/468 [00:28<00:00, 16.17it/s]\n",
      "loss: 0.0206: 100%|██████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "loss: 0.0205: 100%|██████████| 468/468 [00:28<00:00, 16.32it/s]\n",
      "loss: 0.0204: 100%|██████████| 468/468 [00:29<00:00, 15.72it/s]\n",
      "loss: 0.0204: 100%|██████████| 468/468 [00:29<00:00, 15.76it/s]\n",
      "loss: 0.0203: 100%|██████████| 468/468 [00:29<00:00, 15.68it/s]\n",
      "loss: 0.0203: 100%|██████████| 468/468 [00:29<00:00, 15.65it/s]\n",
      "loss: 0.0202: 100%|██████████| 468/468 [00:30<00:00, 15.42it/s]\n",
      "loss: 0.0202: 100%|██████████| 468/468 [00:28<00:00, 16.50it/s]\n",
      "loss: 0.0201: 100%|██████████| 468/468 [00:28<00:00, 16.56it/s]\n",
      "loss: 0.02: 100%|██████████| 468/468 [00:28<00:00, 16.32it/s]  \n",
      "loss: 0.02: 100%|██████████| 468/468 [00:28<00:00, 16.54it/s]\n",
      "loss: 0.0199: 100%|██████████| 468/468 [00:28<00:00, 16.51it/s]\n",
      "loss: 0.0199: 100%|██████████| 468/468 [00:28<00:00, 16.15it/s]\n",
      "loss: 0.0198: 100%|██████████| 468/468 [00:33<00:00, 14.12it/s]\n",
      "loss: 0.0198: 100%|██████████| 468/468 [00:28<00:00, 16.26it/s]\n",
      "loss: 0.0197: 100%|██████████| 468/468 [00:30<00:00, 15.18it/s]\n",
      "loss: 0.0197: 100%|██████████| 468/468 [00:29<00:00, 15.66it/s]\n",
      "loss: 0.0196: 100%|██████████| 468/468 [00:29<00:00, 15.94it/s]\n",
      "loss: 0.0196: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0195: 100%|██████████| 468/468 [00:29<00:00, 15.65it/s]\n",
      "loss: 0.0195: 100%|██████████| 468/468 [00:28<00:00, 16.21it/s]\n",
      "loss: 0.0194: 100%|██████████| 468/468 [00:28<00:00, 16.42it/s]\n",
      "loss: 0.0194: 100%|██████████| 468/468 [00:28<00:00, 16.25it/s]\n",
      "loss: 0.0194: 100%|██████████| 468/468 [00:28<00:00, 16.52it/s]\n",
      "loss: 0.0193: 100%|██████████| 468/468 [00:29<00:00, 16.06it/s]\n",
      "loss: 0.0193: 100%|██████████| 468/468 [00:29<00:00, 15.76it/s]\n",
      "loss: 0.0192: 100%|██████████| 468/468 [00:30<00:00, 15.41it/s]\n",
      "loss: 0.0192: 100%|██████████| 468/468 [00:30<00:00, 15.53it/s]\n",
      "loss: 0.0192: 100%|██████████| 468/468 [00:30<00:00, 15.55it/s]\n",
      "loss: 0.0191: 100%|██████████| 468/468 [00:29<00:00, 15.71it/s]\n",
      "loss: 0.0191: 100%|██████████| 468/468 [00:29<00:00, 16.00it/s]\n",
      "loss: 0.0191: 100%|██████████| 468/468 [00:29<00:00, 15.97it/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p contents_3\n",
    "train_ddpm(ddpm_3, optim_3, dataloader_3, accelerator_3, \"contents_3\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T19:39:28.612906Z",
     "start_time": "2024-03-21T18:28:30.273116Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
