{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOqtixLa1y-b"
   },
   "source": [
    "The following example notebook implements standard diffusion\n",
    "with a simple CNN model to generate realistic MNIST digits.\n",
    "\n",
    "This is a modified implementation of `minDiffusion`\n",
    "which implements [DDPM](https://arxiv.org/abs/2006.11239)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this example notebook,\n",
    "install requirements as in `requirements.txt` (for example, `pip install -r requirements.txt`).\n",
    "You may also wish to follow system-dependent PyTorch instructions\n",
    "[here](https://pytorch.org/) to install accelerated\n",
    "versions of PyTorch, but note they are not needed\n",
    "(I am testing this on my laptop).\n",
    "\n",
    "If you do use accelerated hardware, make sure that your code\n",
    "is still compatible with CPU-only installs.\n",
    "\n",
    "First, let's create a folder to store example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaJ7P2ft2G6j",
    "outputId": "7ce57688-755a-431b-c73d-2e32301824ea",
    "ExecuteTime": {
     "end_time": "2024-03-19T00:22:30.674660Z",
     "start_time": "2024-03-19T00:22:30.540248Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50FGtZsk1y-b",
    "ExecuteTime": {
     "end_time": "2024-03-19T17:41:09.346300Z",
     "start_time": "2024-03-19T17:41:07.359112Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from utils import ddpm_schedules, CNNBlock, CNN, DDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run this on MNIST. We perform some basic preprocessing, and set up the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a6jMrCRa1y-d",
    "ExecuteTime": {
     "end_time": "2024-03-19T17:41:09.374417Z",
     "start_time": "2024-03-19T17:41:09.347606Z"
    }
   },
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our model with a given choice of hidden layers and activation function. We also choose a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-6ApENps1y-d",
    "ExecuteTime": {
     "end_time": "2024-03-19T17:41:11.257859Z",
     "start_time": "2024-03-19T17:41:11.248548Z"
    }
   },
   "outputs": [],
   "source": [
    "gt = CNN(in_channels=1, expected_shape=(28, 28), n_hidden=(16, 32, 32, 16), act=nn.GELU)\n",
    "# For testing: (16, 32, 32, 16)\n",
    "# For more capacity (for example): (64, 128, 256, 128, 64)\n",
    "ddpm = DDPM(gt=gt, betas=(1e-4, 0.02), n_T=1000)\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could set up a GPU if we have one, which is done below.\n",
    "\n",
    "Here, we use HuggingFace's `accelerate` library, which abstracts away all the `.to(device)` calls for us.\n",
    "This lets us focus on the model itself rather than data movement.\n",
    "It also does a few other tricks to speed up calculations.\n",
    "\n",
    "PyTorch Lightning, which we discussed during the course, is another option that also handles a lot more, but is a bit heavyweight.\n",
    "`accelerate` is a simpler option closer to raw PyTorch.\n",
    "However, if you prefer, you could choose to use Lightning for the coursework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T17:41:14.624158Z",
     "start_time": "2024-03-19T17:41:14.569280Z"
    }
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "# We wrap our model, optimizer, and dataloaders with `accelerator.prepare`,\n",
    "# which lets HuggingFace's Accelerate handle the device placement and gradient accumulation.\n",
    "ddpm, optim, dataloader = accelerator.prepare(ddpm, optim, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just make sure this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8wxKbzEa1y-e",
    "ExecuteTime": {
     "end_time": "2024-03-19T17:41:21.720249Z",
     "start_time": "2024-03-19T17:41:16.618729Z"
    }
   },
   "outputs": [],
   "source": [
    "for x, _ in dataloader:\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    ddpm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train it. You can exit early by interrupting the kernel. Images\n",
    "are saved to the `contents` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLiE8x-c1y-e",
    "outputId": "a9f81c32-96c2-4e3b-cee9-fd2d2d4e316c",
    "ExecuteTime": {
     "end_time": "2024-03-19T18:36:09.091242Z",
     "start_time": "2024-03-19T17:41:28.914123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.132: 100%|██████████| 468/468 [00:34<00:00, 13.69it/s]\n",
      "loss: 0.089: 100%|██████████| 468/468 [00:30<00:00, 15.48it/s] \n",
      "loss: 0.072: 100%|██████████| 468/468 [00:29<00:00, 16.05it/s] \n",
      "loss: 0.0623: 100%|██████████| 468/468 [00:29<00:00, 15.96it/s]\n",
      "loss: 0.056: 100%|██████████| 468/468 [00:29<00:00, 15.93it/s] \n",
      "loss: 0.0514: 100%|██████████| 468/468 [00:29<00:00, 15.94it/s]\n",
      "loss: 0.0479: 100%|██████████| 468/468 [00:29<00:00, 15.73it/s]\n",
      "loss: 0.0453: 100%|██████████| 468/468 [00:30<00:00, 15.53it/s]\n",
      "loss: 0.0431: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0412: 100%|██████████| 468/468 [00:28<00:00, 16.24it/s]\n",
      "loss: 0.0397: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0384: 100%|██████████| 468/468 [00:29<00:00, 15.71it/s]\n",
      "loss: 0.0372: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0362: 100%|██████████| 468/468 [00:28<00:00, 16.64it/s]\n",
      "loss: 0.0353: 100%|██████████| 468/468 [00:28<00:00, 16.61it/s]\n",
      "loss: 0.0344: 100%|██████████| 468/468 [00:28<00:00, 16.63it/s]\n",
      "loss: 0.0337: 100%|██████████| 468/468 [00:27<00:00, 16.81it/s]\n",
      "loss: 0.033: 100%|██████████| 468/468 [00:27<00:00, 16.72it/s] \n",
      "loss: 0.0324: 100%|██████████| 468/468 [00:27<00:00, 16.89it/s]\n",
      "loss: 0.0318: 100%|██████████| 468/468 [00:28<00:00, 16.37it/s]\n",
      "loss: 0.0313: 100%|██████████| 468/468 [00:28<00:00, 16.64it/s]\n",
      "loss: 0.0309: 100%|██████████| 468/468 [00:28<00:00, 16.53it/s]\n",
      "loss: 0.0304: 100%|██████████| 468/468 [00:28<00:00, 16.32it/s]\n",
      "loss: 0.03: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]  \n",
      "loss: 0.0296: 100%|██████████| 468/468 [00:28<00:00, 16.25it/s]\n",
      "loss: 0.0292: 100%|██████████| 468/468 [00:28<00:00, 16.48it/s]\n",
      "loss: 0.0289: 100%|██████████| 468/468 [00:27<00:00, 16.72it/s]\n",
      "loss: 0.0286: 100%|██████████| 468/468 [00:28<00:00, 16.39it/s]\n",
      "loss: 0.0283: 100%|██████████| 468/468 [00:28<00:00, 16.39it/s]\n",
      "loss: 0.028: 100%|██████████| 468/468 [00:27<00:00, 16.82it/s] \n",
      "loss: 0.0277: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0275: 100%|██████████| 468/468 [00:29<00:00, 16.06it/s]\n",
      "loss: 0.0272: 100%|██████████| 468/468 [00:28<00:00, 16.21it/s]\n",
      "loss: 0.027: 100%|██████████| 468/468 [00:29<00:00, 16.01it/s] \n",
      "loss: 0.0268: 100%|██████████| 468/468 [00:28<00:00, 16.44it/s]\n",
      "loss: 0.0265: 100%|██████████| 468/468 [00:28<00:00, 16.33it/s]\n",
      "loss: 0.0263: 100%|██████████| 468/468 [00:28<00:00, 16.40it/s]\n",
      "loss: 0.0262: 100%|██████████| 468/468 [00:28<00:00, 16.41it/s]\n",
      "loss: 0.026: 100%|██████████| 468/468 [00:29<00:00, 15.91it/s] \n",
      "loss: 0.0258: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0256: 100%|██████████| 468/468 [00:29<00:00, 15.96it/s]\n",
      "loss: 0.0254: 100%|██████████| 468/468 [00:28<00:00, 16.28it/s]\n",
      "loss: 0.0253: 100%|██████████| 468/468 [00:28<00:00, 16.29it/s]\n",
      "loss: 0.0251: 100%|██████████| 468/468 [00:28<00:00, 16.40it/s]\n",
      "loss: 0.025: 100%|██████████| 468/468 [00:28<00:00, 16.15it/s] \n",
      "loss: 0.0248: 100%|██████████| 468/468 [00:28<00:00, 16.58it/s]\n",
      "loss: 0.0247: 100%|██████████| 468/468 [00:28<00:00, 16.41it/s]\n",
      "loss: 0.0246: 100%|██████████| 468/468 [00:28<00:00, 16.57it/s]\n",
      "loss: 0.0244: 100%|██████████| 468/468 [00:28<00:00, 16.38it/s]\n",
      "loss: 0.0243: 100%|██████████| 468/468 [00:28<00:00, 16.34it/s]\n",
      "loss: 0.0242: 100%|██████████| 468/468 [00:29<00:00, 16.14it/s]\n",
      "loss: 0.0241: 100%|██████████| 468/468 [00:29<00:00, 16.04it/s]\n",
      "loss: 0.024: 100%|██████████| 468/468 [00:29<00:00, 16.00it/s]\n",
      "loss: 0.0239: 100%|██████████| 468/468 [00:29<00:00, 15.79it/s]\n",
      "loss: 0.0237: 100%|██████████| 468/468 [00:28<00:00, 16.25it/s]\n",
      "loss: 0.0236: 100%|██████████| 468/468 [00:28<00:00, 16.20it/s]\n",
      "loss: 0.0235: 100%|██████████| 468/468 [00:29<00:00, 15.92it/s]\n",
      "loss: 0.0234: 100%|██████████| 468/468 [00:28<00:00, 16.19it/s]\n",
      "loss: 0.0234: 100%|██████████| 468/468 [00:30<00:00, 15.51it/s]\n",
      "loss: 0.0233: 100%|██████████| 468/468 [00:30<00:00, 15.59it/s]\n",
      "loss: 0.0232: 100%|██████████| 468/468 [01:44<00:00,  4.49it/s]\n",
      "loss: 0.0231: 100%|██████████| 468/468 [00:29<00:00, 15.79it/s]\n",
      "loss: 0.023: 100%|██████████| 468/468 [00:30<00:00, 15.52it/s] \n",
      "loss: 0.0229: 100%|██████████| 468/468 [00:29<00:00, 15.90it/s]\n",
      "loss: 0.0228: 100%|██████████| 468/468 [00:29<00:00, 16.06it/s]\n",
      "loss: 0.0228: 100%|██████████| 468/468 [00:29<00:00, 15.94it/s]\n",
      "loss: 0.0227: 100%|██████████| 468/468 [00:29<00:00, 16.08it/s]\n",
      "loss: 0.0226: 100%|██████████| 468/468 [00:28<00:00, 16.17it/s]\n",
      "loss: 0.0225: 100%|██████████| 468/468 [00:28<00:00, 16.32it/s]\n",
      "loss: 0.0225: 100%|██████████| 468/468 [00:28<00:00, 16.30it/s]\n",
      "loss: 0.0224: 100%|██████████| 468/468 [00:28<00:00, 16.36it/s]\n",
      "loss: 0.0223: 100%|██████████| 468/468 [00:28<00:00, 16.33it/s]\n",
      "loss: 0.0223: 100%|██████████| 468/468 [00:28<00:00, 16.43it/s]\n",
      "loss: 0.0222: 100%|██████████| 468/468 [00:28<00:00, 16.24it/s]\n",
      "loss: 0.0221: 100%|██████████| 468/468 [00:29<00:00, 16.07it/s]\n",
      "loss: 0.0221: 100%|██████████| 468/468 [00:30<00:00, 15.44it/s]\n",
      "loss: 0.022: 100%|██████████| 468/468 [00:30<00:00, 15.59it/s] \n",
      "loss: 0.022: 100%|██████████| 468/468 [00:30<00:00, 15.46it/s]\n",
      "loss: 0.0219: 100%|██████████| 468/468 [00:30<00:00, 15.52it/s]\n",
      "loss: 0.0219: 100%|██████████| 468/468 [00:29<00:00, 15.66it/s]\n",
      "loss: 0.0218: 100%|██████████| 468/468 [00:29<00:00, 15.71it/s]\n",
      "loss: 0.0217: 100%|██████████| 468/468 [00:30<00:00, 15.45it/s]\n",
      "loss: 0.0217: 100%|██████████| 468/468 [00:30<00:00, 15.30it/s]\n",
      "loss: 0.0216: 100%|██████████| 468/468 [00:31<00:00, 15.06it/s]\n",
      "loss: 0.0216: 100%|██████████| 468/468 [00:29<00:00, 15.65it/s]\n",
      "loss: 0.0215: 100%|██████████| 468/468 [00:30<00:00, 15.22it/s]\n",
      "loss: 0.0215: 100%|██████████| 468/468 [00:30<00:00, 15.15it/s]\n",
      "loss: 0.0214: 100%|██████████| 468/468 [00:30<00:00, 15.37it/s]\n",
      "loss: 0.0214: 100%|██████████| 468/468 [00:29<00:00, 15.64it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:30<00:00, 15.34it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:30<00:00, 15.21it/s]\n",
      "loss: 0.0213: 100%|██████████| 468/468 [00:28<00:00, 16.15it/s]\n",
      "loss: 0.0212: 100%|██████████| 468/468 [00:29<00:00, 16.09it/s]\n",
      "loss: 0.0212: 100%|██████████| 468/468 [00:30<00:00, 15.56it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:29<00:00, 15.80it/s]\n",
      "loss: 0.0211: 100%|██████████| 468/468 [00:30<00:00, 15.40it/s]\n",
      "loss: 0.021: 100%|██████████| 468/468 [00:29<00:00, 15.67it/s] \n",
      "loss: 0.021: 100%|██████████| 468/468 [00:29<00:00, 15.93it/s]\n",
      "loss: 0.021: 100%|██████████| 468/468 [00:29<00:00, 15.61it/s]\n",
      "loss: 0.0209: 100%|██████████| 468/468 [00:29<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "losses = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    ddpm.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)  # Wrap our loop with a visual progress bar\n",
    "    for x, _ in pbar:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = ddpm(x)\n",
    "\n",
    "        loss.backward()\n",
    "        # ^Technically should be `accelerator.backward(loss)` but not necessary for local training\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        avg_loss = np.average(losses[min(len(losses)-100, 0):])\n",
    "        pbar.set_description(f\"loss: {avg_loss:.3g}\")  # Show running average of loss in progress bar\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        xh = ddpm.sample(16, (1, 28, 28), accelerator.device)  # Can get device explicitly with `accelerator.device`\n",
    "        grid = make_grid(xh, nrow=4)\n",
    "\n",
    "        # Save samples to `./contents` directory\n",
    "        save_image(grid, f\"./contents/ddpm_sample_{i:04d}.png\")\n",
    "\n",
    "        # save model\n",
    "        torch.save(ddpm.state_dict(), f\"./contents/ddpm_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
